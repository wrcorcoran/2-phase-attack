{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adad264-33ed-423b-a05b-da9ec854c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb76d695-ece8-4cc7-8ffc-c5e49fe04d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data_collection\"\n",
    "output_folder = \"tables\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "greedy_data = []\n",
    "mcmc_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573b376-1860-4527-b0c4-22a385143728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4061f8fc-2d7a-455d-b58a-414d81cde7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to tables/results_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Path to the data collection directory\n",
    "data_dir = \"data_collection\"  # Update if necessary\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# File to exclude\n",
    "exclude_file = \"mcmc_gcn_cora_constant_binary_results.pkl\"\n",
    "\n",
    "# Process each pickle file\n",
    "for file_name in os.listdir(data_dir):\n",
    "    if file_name.endswith(\".pkl\") and file_name != exclude_file:\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            recording = pickle.load(file)\n",
    "\n",
    "        # Check if is_reserved exists, skip file if missing\n",
    "        if not hasattr(recording, \"is_reserved\"):\n",
    "            print(f\"Skipping {file_name} due to missing 'is_reserved' attribute.\")\n",
    "            continue  # Skip this file\n",
    "        \n",
    "        # Check if accuracies and losses are non-empty and properly formatted\n",
    "        if not recording.accuracies or not isinstance(recording.accuracies, dict):\n",
    "            print(f\"Skipping {file_name} due to invalid or empty accuracies.\")\n",
    "            continue\n",
    "\n",
    "        if not recording.losses or not isinstance(recording.losses, dict):\n",
    "            print(f\"Skipping {file_name} due to invalid or empty losses.\")\n",
    "            continue\n",
    "\n",
    "        # Extract final accuracy and loss\n",
    "        try:\n",
    "            final_accuracy = None\n",
    "            for key in sorted(recording.accuracies.keys(), reverse=True):  # Get the last accuracy list\n",
    "                if recording.accuracies[key]:\n",
    "                    final_accuracy = recording.accuracies[key][-1]\n",
    "                    break\n",
    "\n",
    "            final_loss = None\n",
    "            for key in sorted(recording.losses.keys(), reverse=True):  # Get the last loss list\n",
    "                if recording.losses[key]:\n",
    "                    final_loss = recording.losses[key][-1]\n",
    "                    break\n",
    "\n",
    "        except IndexError:\n",
    "            print(f\"Skipping {file_name} due to IndexError in extracting accuracy/loss.\")\n",
    "            continue\n",
    "\n",
    "        total_iterations = sum(recording.iterations.values()) if recording.iterations else 0\n",
    "\n",
    "        # Append a row to the results\n",
    "        results.append({\n",
    "            \"File Name\": file_name,\n",
    "            \"Model\": recording.model.name,\n",
    "            \"Dataset\": recording.dataset.name,\n",
    "            \"Accept Fn\": recording.accept.name,\n",
    "            \"Select Fn\": recording.select.name,\n",
    "            \"Reverse Type\": recording.is_reserved.name,  # Ensure it's an Enum\n",
    "            \"Final Accuracy\": final_accuracy if final_accuracy is not None else \"N/A\",\n",
    "            \"Final Loss\": final_loss if final_loss is not None else \"N/A\",\n",
    "            \"Total Iterations\": total_iterations,\n",
    "        })\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV file\n",
    "output_file = \"tables/results_summary.csv\"\n",
    "os.makedirs(\"tables\", exist_ok=True)  # Ensure the directory exists\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea90a38f-6a44-4fe0-901c-1b96f266081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files missing the 'is_reserved' attribute:\n",
      "- mcmc_gcn_cora_constant_binary_results.pkl\n"
     ]
    }
   ],
   "source": [
    "##This function is to find the file that is missing is_reserved in the attribute\n",
    "\n",
    "missing_is_reserved = []\n",
    "\n",
    "# Process each pickle file\n",
    "for file_name in os.listdir(data_folder):\n",
    "    if file_name.endswith(\".pkl\"):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            recording = pickle.load(file)\n",
    "        \n",
    "        # Check if is_reserved exists\n",
    "        if not hasattr(recording, \"is_reserved\"):\n",
    "            missing_is_reserved.append(file_name)\n",
    "\n",
    "# Print results\n",
    "if missing_is_reserved:\n",
    "    print(\"Files missing the 'is_reserved' attribute:\")\n",
    "    for file in missing_is_reserved:\n",
    "        print(f\"- {file}\")\n",
    "else:\n",
    "    print(\"All files contain the 'is_reserved' attribute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "298af98f-5517-4a16-8fb3-bad777c9be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy results saved to tables/greedy_results.csv\n",
      "MCMC results saved to tables/mcmc_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "data_dir = \"data_collection\"  # Folder where all .pkl files are stored\n",
    "output_dir = \"tables\"  # Folder where results are saved\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# File to exclude\n",
    "exclude_file = \"mcmc_gcn_cora_constant_binary_results.pkl\"\n",
    "\n",
    "# Lists to store results separately\n",
    "greedy_results = []\n",
    "mcmc_results = []\n",
    "\n",
    "# Process each pickle file\n",
    "for file_name in os.listdir(data_dir):\n",
    "    if file_name.endswith(\".pkl\") and file_name != exclude_file:\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            recording = pickle.load(file)\n",
    "\n",
    "        # Check if is_reserved exists, skip file if missing\n",
    "        if not hasattr(recording, \"is_reserved\"):\n",
    "            print(f\"Skipping {file_name} due to missing 'is_reserved' attribute.\")\n",
    "            continue  # Skip this file\n",
    "        \n",
    "        # Check if accuracies and losses are non-empty and properly formatted\n",
    "        if not recording.accuracies or not isinstance(recording.accuracies, dict):\n",
    "            print(f\"Skipping {file_name} due to invalid or empty accuracies.\")\n",
    "            continue\n",
    "\n",
    "        if not recording.losses or not isinstance(recording.losses, dict):\n",
    "            print(f\"Skipping {file_name} due to invalid or empty losses.\")\n",
    "            continue\n",
    "\n",
    "        # Extract initial and final accuracy\n",
    "        try:\n",
    "            initial_accuracy = None\n",
    "            final_accuracy = None\n",
    "            for key in sorted(recording.accuracies.keys()):  # Get first non-empty list\n",
    "                if recording.accuracies[key]:\n",
    "                    initial_accuracy = recording.accuracies[key][0]\n",
    "                    break\n",
    "\n",
    "            for key in sorted(recording.accuracies.keys(), reverse=True):  # Get last non-empty list\n",
    "                if recording.accuracies[key]:\n",
    "                    final_accuracy = recording.accuracies[key][-1]\n",
    "                    break\n",
    "\n",
    "            # Extract initial and final loss\n",
    "            initial_loss = None\n",
    "            final_loss = None\n",
    "            for key in sorted(recording.losses.keys()):  # Get first non-empty list\n",
    "                if recording.losses[key]:\n",
    "                    initial_loss = round(recording.losses[key][0], 5)\n",
    "                    break\n",
    "\n",
    "            for key in sorted(recording.losses.keys(), reverse=True):  # Get last non-empty list\n",
    "                if recording.losses[key]:\n",
    "                    final_loss = round(recording.losses[key][-1], 5)  # Round to 3 decimal places\n",
    "                    break\n",
    "\n",
    "        except IndexError:\n",
    "            print(f\"Skipping {file_name} due to IndexError in extracting accuracy/loss.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare result row\n",
    "        result_row = {\n",
    "            \"File Name\": file_name,\n",
    "            \"Model\": recording.model.name,\n",
    "            \"Dataset\": recording.dataset.name,\n",
    "            \"Accept Fn\": recording.accept.name,\n",
    "            \"Select Fn\": recording.select.name,\n",
    "            \"Reverse Type\": recording.is_reserved.name,  # Ensure it's an Enum\n",
    "            \"Initial Accuracy\": initial_accuracy if initial_accuracy is not None else \"N/A\",\n",
    "            \"Final Accuracy\": final_accuracy if final_accuracy is not None else \"N/A\",\n",
    "            \"Initial Loss\": initial_loss if initial_loss is not None else \"N/A\",\n",
    "            \"Final Loss\": final_loss if final_loss is not None else \"N/A\",\n",
    "        }\n",
    "\n",
    "        # Categorize into Greedy or MCMC\n",
    "        if \"greedy\" in file_name.lower():\n",
    "            greedy_results.append(result_row)\n",
    "        elif \"mcmc\" in file_name.lower():\n",
    "            mcmc_results.append(result_row)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 20)  # Truncate long text\n",
    "pd.set_option(\"display.width\", 100)  # Reduce overall table width\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_greedy = pd.DataFrame(greedy_results)\n",
    "df_mcmc = pd.DataFrame(mcmc_results)\n",
    "\n",
    "# Save to CSV files\n",
    "greedy_output_file = os.path.join(output_dir, \"greedy_results.csv\")\n",
    "mcmc_output_file = os.path.join(output_dir, \"mcmc_results.csv\")\n",
    "\n",
    "df_greedy.to_csv(greedy_output_file, index=False)\n",
    "df_mcmc.to_csv(mcmc_output_file, index=False)\n",
    "\n",
    "print(f\"Greedy results saved to {greedy_output_file}\")\n",
    "print(f\"MCMC results saved to {mcmc_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c1828d-de19-433c-a288-38a516c5540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: tables/formatted_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define path to directory\n",
    "data_path = \"data_collection\"  # Update this if needed\n",
    "output_dir = \"tables\"\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "\n",
    "# Process each pickle file\n",
    "for file_name in os.listdir(data_path):\n",
    "    if not file_name.endswith(\".pkl\"):\n",
    "        continue  # Skip non-pickle files\n",
    "    \n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        recording = pickle.load(file)\n",
    "\n",
    "    # Ensure is_reserved attribute exists\n",
    "    if not hasattr(recording, \"is_reserved\"):\n",
    "        continue  # Skip files without this attribute\n",
    "    \n",
    "    # Extract splits\n",
    "    for split, accuracy_list in recording.accuracies.items():\n",
    "        if len(accuracy_list) == 0:\n",
    "            continue  # Skip empty splits\n",
    "\n",
    "        # Extract initial and final values\n",
    "        initial_accuracy = accuracy_list[0]\n",
    "        final_accuracy = accuracy_list[-1]\n",
    "        initial_loss = recording.losses[split][0] if split in recording.losses else None\n",
    "        final_loss = round(recording.losses[split][-1], 5) if split in recording.losses else None\n",
    "\n",
    "        # Append results in hierarchical format\n",
    "        results.append([\n",
    "            recording.dataset.name,  # Dataset\n",
    "            recording.model.name,    # Model\n",
    "            recording.accept.name,   # Accept Function\n",
    "            recording.select.name,   # Select Function\n",
    "            split,                   # Split values (0.0, 0.5, 0.7, 0.9)\n",
    "            recording.is_reserved.name,  # Reverse Type\n",
    "            round(initial_accuracy, 5),\n",
    "            round(final_accuracy, 5),\n",
    "            round(initial_loss, 5) if initial_loss is not None else None,\n",
    "            round(final_loss, 5) if final_loss is not None else None\n",
    "        ])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    \"Dataset\", \"Model\", \"Accept Fn\", \"Select Fn\", \"Split\",\n",
    "    \"Reverse Type\", \"Initial Accuracy\", \"Final Accuracy\", \"Initial Loss\", \"Final Loss\"\n",
    "])\n",
    "\n",
    "# Pivot Table to Match the Required Layout\n",
    "pivot_df = df.pivot_table(\n",
    "    index=[\"Dataset\", \"Model\"],\n",
    "    columns=[\"Accept Fn\", \"Select Fn\", \"Split\"],\n",
    "    values=[\"Final Accuracy\"],\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "\n",
    "# Reset index to make it CSV-friendly\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = os.path.join(output_dir, \"formatted_results.csv\")\n",
    "pivot_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved at: {csv_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6972f73-5996-43d2-a85e-d1affa930fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: tables/formatted_results_2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define path to directory\n",
    "data_path = \"data_collection\"  # Update this if needed\n",
    "output_dir = \"tables\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "\n",
    "# Process each pickle file\n",
    "for file_name in os.listdir(data_path):\n",
    "    if not file_name.endswith(\".pkl\"):\n",
    "        continue  # Skip non-pickle files\n",
    "    \n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        recording = pickle.load(file)\n",
    "\n",
    "    # Ensure is_reserved attribute exists\n",
    "    if not hasattr(recording, \"is_reserved\"):\n",
    "        continue  # Skip files without this attribute\n",
    "    \n",
    "    # Extract splits\n",
    "    for split, accuracy_list in recording.accuracies.items():\n",
    "        if len(accuracy_list) == 0:\n",
    "            continue  # Skip empty splits\n",
    "\n",
    "        # Extract initial and final values\n",
    "        initial_accuracy = accuracy_list[0]\n",
    "        final_accuracy = accuracy_list[-1]\n",
    "        initial_loss = recording.losses[split][0] if split in recording.losses else None\n",
    "        final_loss = round(recording.losses[split][-1], 5) if split in recording.losses else None\n",
    "\n",
    "        # Append results in hierarchical format\n",
    "        results.append([\n",
    "            recording.dataset.name,  # Dataset\n",
    "            recording.model.name,    # Model\n",
    "            recording.accept.name,   # Accept Function\n",
    "            recording.select.name,   # Select Function\n",
    "            split,                   # Split values (0.0, 0.5, 0.7, 0.9)\n",
    "            recording.is_reserved.name,  # Reverse Type\n",
    "            round(initial_accuracy, 5),\n",
    "            round(final_accuracy, 5),\n",
    "            round(initial_loss, 5) if initial_loss is not None else None,\n",
    "            round(final_loss, 5) if final_loss is not None else None\n",
    "        ])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results, columns=[\n",
    "    \"Dataset\", \"Model\", \"Accept Fn\", \"Select Fn\", \"Split\",\n",
    "    \"Reverse Type\", \"Initial Accuracy\", \"Final Accuracy\", \"Initial Loss\", \"Final Loss\"\n",
    "])\n",
    "\n",
    "# Pivot Table for Correct Formatting\n",
    "pivot_df = df.pivot_table(\n",
    "    index=[\"Dataset\", \"Model\"],  # Rows: Dataset (CORA, CITESEER) â†’ Model (GCN, GAT, GSAGE)\n",
    "    columns=[\"Accept Fn\", \"Select Fn\", \"Split\"],  # Hierarchical columns\n",
    "    values=\"Final Accuracy\",\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "# Properly Format MultiIndex Column Names\n",
    "pivot_df.columns = pd.MultiIndex.from_tuples(pivot_df.columns, names=[\"Accept Fn\", \"Select Fn\", \"Split\"])\n",
    "\n",
    "# Reset index but keep MultiIndex columns\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# Save CSV with Proper MultiIndex Formatting\n",
    "csv_path = os.path.join(output_dir, \"formatted_results_2.csv\")\n",
    "pivot_df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"CSV file saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09adde0-d20d-4544-a257-66fa2a3f97e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: tables/mcmc_formatted_results.csv\n",
      "CSV file saved at: tables/greedy_formatted_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define path to directory\n",
    "data_path = \"data_collection\"  # Update this if needed\n",
    "output_dir = \"tables\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Storage for results\n",
    "mcmc_results = []\n",
    "greedy_results = []\n",
    "\n",
    "# Process each pickle file\n",
    "for file_name in os.listdir(data_path):\n",
    "    if not file_name.endswith(\".pkl\"):\n",
    "        continue  # Skip non-pickle files\n",
    "    \n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        recording = pickle.load(file)\n",
    "\n",
    "    # Ensure is_reserved attribute exists\n",
    "    if not hasattr(recording, \"is_reserved\"):\n",
    "        continue  # Skip files without this attribute\n",
    "\n",
    "    # Determine if the file belongs to MCMC or Greedy\n",
    "    if \"mcmc\" in file_name.lower():\n",
    "        results_list = mcmc_results\n",
    "    elif \"greedy\" in file_name.lower():\n",
    "        results_list = greedy_results\n",
    "    else:\n",
    "        continue  # Skip files that don't belong to either category\n",
    "\n",
    "    # Extract splits\n",
    "    for split, accuracy_list in recording.accuracies.items():\n",
    "        if len(accuracy_list) == 0:\n",
    "            continue  \n",
    "\n",
    "        \n",
    "        initial_accuracy = accuracy_list[0]\n",
    "        final_accuracy = accuracy_list[-1]\n",
    "        initial_loss = recording.losses[split][0] if split in recording.losses else None\n",
    "        final_loss = round(recording.losses[split][-1], 5) if split in recording.losses else None\n",
    "\n",
    "        \n",
    "        if initial_loss is not None and final_loss is not None and initial_loss != 0:\n",
    "            loss_change = round((final_loss - initial_loss) / initial_loss, 5)\n",
    "        else:\n",
    "            loss_change = None  \n",
    "\n",
    "        # Append results in hierarchical format\n",
    "        results_list.append([\n",
    "            recording.dataset.name,  # Dataset\n",
    "            recording.model.name,    # Model\n",
    "            recording.accept.name,   # Accept Function\n",
    "            recording.select.name,   # Select Function\n",
    "            split,                   # Split values (0.0, 0.5, 0.7, 0.9)\n",
    "            recording.is_reserved.name,  # Reverse Type\n",
    "            loss_change\n",
    "        ])\n",
    "\n",
    "\n",
    "def save_formatted_table(results, filename):\n",
    "    if not results:\n",
    "        print(f\"No data for {filename}, skipping...\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(results, columns=[\n",
    "        \"Dataset\", \"Model\", \"Accept Fn\", \"Select Fn\", \"Split\",\n",
    "        \"Reverse Type\", \"Loss Change\"\n",
    "    ])\n",
    "\n",
    "    \n",
    "    pivot_df = df.pivot_table(\n",
    "        index=[\"Dataset\", \"Model\"],  # Rows: Dataset (CORA, CITESEER) -> Model (GCN, GAT, GSAGE)\n",
    "        columns=[\"Accept Fn\", \"Select Fn\", \"Split\"],  # Hierarchical columns\n",
    "        values=\"Loss Change\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    pivot_df.columns = pd.MultiIndex.from_tuples(pivot_df.columns, names=[\"Accept Fn\", \"Select Fn\", \"Split\"])\n",
    "    pivot_df.reset_index(inplace=True)\n",
    "\n",
    "    \n",
    "    csv_path = os.path.join(output_dir, filename)\n",
    "    pivot_df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"CSV file saved at: {csv_path}\")\n",
    "\n",
    "\n",
    "save_formatted_table(mcmc_results, \"mcmc_formatted_results.csv\")\n",
    "save_formatted_table(greedy_results, \"greedy_formatted_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339912ee-35f5-46dd-a624-3d8d5adc028c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
