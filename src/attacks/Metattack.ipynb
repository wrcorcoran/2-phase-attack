{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2d5018-6c9f-4c8f-b536-1e12dad31187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import count\n",
    "from torch import autograd\n",
    "import copy\n",
    "\n",
    "sys.path.append('../')\n",
    "from models.gcn import *\n",
    "from utils.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb35dfee-91ef-455a-8606-8c30a758bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_cora(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbacf27c-813f-44d6-b28b-24c87fb24498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, h_feats)\n",
    "        self.conv2 = GCNConv(h_feats, num_classes)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        return h\n",
    "\n",
    "    def forward_with_adj(self, x, adj):\n",
    "        # Use A directly in matrix multiplication\n",
    "        h = torch.mm(adj, self.conv1(x))  # Dense adjacency for feature propagation\n",
    "        h = F.relu(h)\n",
    "        h = torch.mm(adj, self.conv2(h))\n",
    "        return h\n",
    "\n",
    "    def train_model(self, data, differentiable=False):\n",
    "        self.train()\n",
    "        logits = self(data.cuda())\n",
    "        loss = F.cross_entropy(logits[data.train_mask], data.y[data.train_mask]) \n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        if differentiable:\n",
    "            grads = torch.autograd.grad(loss, self.parameters(), create_graph=True)\n",
    "            for param, grad in zip(self.parameters(), grads):\n",
    "                param.grad = grad\n",
    "        else:\n",
    "            loss.backward()\n",
    "    \n",
    "        self.optimizer.step()\n",
    "    \n",
    "        return loss.item()\n",
    "\n",
    "    def test(self, data):\n",
    "        self.eval()\n",
    "        out = self(data.cuda())\n",
    "        pred = out.argmax(dim=1)\n",
    "    \n",
    "        acc = (pred[data.test_mask] == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
    "        return acc\n",
    "\n",
    "    def fit(self, data, epochs=200, **kwargs):\n",
    "        for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "            loss = self.train_model(data, **kwargs)\n",
    "            acc = self.test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e1499b-9708-4877-89da-af3bfa8edade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj\n",
    "import torch\n",
    "\n",
    "def compute_adj_gradient(model, data):\n",
    "    # Convert edge_index to a dense adjacency matrix and set requires_grad=True\n",
    "    A = to_dense_adj(data.edge_index)[0].float().to(data.x.device)\n",
    "    A.requires_grad_(True)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass with A used directly in the model\n",
    "    output = model.forward_with_adj(data.x, A)\n",
    "    \n",
    "    # Compute the loss with respect to the training set\n",
    "    loss = F.cross_entropy(output[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    # Use torch.autograd.grad to compute gradients of loss with respect to A, allowing unused\n",
    "    adj_grad = torch.autograd.grad(loss, A, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "\n",
    "    # Check if adj_grad is None (i.e., A was not used in a differentiable way)\n",
    "    if adj_grad is None:\n",
    "        print(\"Warning: A was not used in a differentiable way in the model.\")\n",
    "    else:\n",
    "        adj_grad = adj_grad.clone().detach()  # Clone and detach to avoid further backprop issues\n",
    "\n",
    "    return adj_grad, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135484e4-21c2-4ec6-8aee-d1e51eb4f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def identify_perturbations(adj_grad, A, num_phase_1, num_phase_2):\n",
    "    # Flatten the gradient matrix for easy sorting\n",
    "    grad_flat = adj_grad.cpu().numpy().flatten()\n",
    "    A_flat = A.detach().cpu().numpy().flatten()\n",
    "    \n",
    "    # Find indices of edges (1s) and non-edges (0s)\n",
    "    non_edge_indices = np.where(A_flat == 0)[0]\n",
    "    edge_indices = np.where(A_flat == 1)[0]\n",
    "    \n",
    "    # Sort non-edges by gradient (least impactful first)\n",
    "    least_impactful_indices = non_edge_indices[np.argsort(grad_flat[non_edge_indices])][:num_phase_1]\n",
    "    \n",
    "    # Sort edges by gradient (most impactful last)\n",
    "    most_impactful_indices = edge_indices[np.argsort(-grad_flat[edge_indices])][:num_phase_2]\n",
    "    \n",
    "    return least_impactful_indices, most_impactful_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5a5e04-1224-4603-8dec-d2131368aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "def apply_perturbations(A, perturb_indices):\n",
    "    A_flat = A.view(-1)  # Flatten A to apply perturbations\n",
    "    A_flat[perturb_indices] = 1 - A_flat[perturb_indices]  # Flip edges\n",
    "    return A.view(A.shape)  # Reshape back to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c93a8a-3f6c-426b-9744-48ad3e9f0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming `data` is already loaded and contains `x` (features) and `y` (labels)\n",
    "in_feats = data.x.shape[1]\n",
    "num_classes = int(data.y.max().item()) + 1\n",
    "\n",
    "# Initialize the model\n",
    "model = GCN(in_feats=in_feats, h_feats=64, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0701f59d-36e9-4db3-9c29-30e706a37abd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m num_phase_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Most impactful (Phase 2)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 3.1: Compute gradients\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m adj_grad, A_dense \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_adj_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 3.2: Identify edges to perturb\u001b[39;00m\n\u001b[1;32m      9\u001b[0m least_impactful_indices, most_impactful_indices \u001b[38;5;241m=\u001b[39m identify_perturbations(adj_grad, A_dense, num_phase_1, num_phase_2)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mcompute_adj_gradient\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass with A used directly in the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute the loss with respect to the training set\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mGCN.forward_with_adj\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_with_adj\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Use A directly in matrix multiplication\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(adj, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Dense adjacency for feature propagation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(h)\n\u001b[1;32m     27\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(adj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h))\n",
      "File \u001b[0;32m~/miniconda3/envs/ersp_v2/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'edge_index'"
     ]
    }
   ],
   "source": [
    "# Number of edges to perturb in each phase\n",
    "num_phase_1 = 200  # Least impactful (Phase 1)\n",
    "num_phase_2 = 100  # Most impactful (Phase 2)\n",
    "\n",
    "# Step 3.1: Compute gradients\n",
    "adj_grad, A_dense = compute_adj_gradient(model, data)\n",
    "\n",
    "# Step 3.2: Identify edges to perturb\n",
    "least_impactful_indices, most_impactful_indices = identify_perturbations(adj_grad, A_dense, num_phase_1, num_phase_2)\n",
    "\n",
    "# Step 3.3: Apply Phase 1 perturbations\n",
    "A_phase_1 = apply_perturbations(A_dense.clone(), least_impactful_indices)\n",
    "\n",
    "# Convert A_phase_1 back to edge_index format if needed\n",
    "data.edge_index = A_phase_1.to_sparse().indices()\n",
    "\n",
    "# Step 3.4: Evaluate after Phase 1 perturbations\n",
    "accuracy_phase_1 = model.test(data)\n",
    "print(f'Accuracy after Phase 1 perturbations: {accuracy_phase_1:.4f}')\n",
    "\n",
    "# Step 3.5: Apply Phase 2 perturbations\n",
    "A_phase_2 = apply_perturbations(A_phase_1.clone(), most_impactful_indices)\n",
    "\n",
    "# Convert A_phase_2 back to edge_index format if needed\n",
    "data.edge_index = A_phase_2.to_sparse().indices()\n",
    "\n",
    "# Step 3.6: Evaluate after Phase 2 perturbations\n",
    "accuracy_phase_2 = model.test(data)\n",
    "print(f'Accuracy after Phase 2 perturbations: {accuracy_phase_2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eef8ff-0c4e-4935-98b2-f7ea752361ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
